# -*- coding: utf-8 -*-
"""Barzin_Zanganeh_Movie_Agent_RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_whjEX7_ZoSdqtCkxhJVMs3Ou9f6obuS

# üé¨ IMDb Movie Recommendation Chatbot
### Intelligent Conversational AI for Movie Discovery

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![LangChain](https://img.shields.io/badge/LangChain-Framework-green)
![OpenAI](https://img.shields.io/badge/OpenAI-GPT--4-orange)
![Gradio](https://img.shields.io/badge/UI-Gradio-yellow)

---

## üìã Project Information

| **Attribute** | **Details** |
| :--- | :--- |
| **Author** | Alireza Barzin Zanganeh |
| **Contact** | abarzinzanganeh@gmail.com |
| **Date** | December 14, 2025 |
| **Project Type** | Case Study 2 - Agentic AI & RAG System |

---

## Problem Statement

The primary objective was to engineer an intelligent chatbot capable of leveraging Natural Language Processing (NLP) and Large Language Models (LLMs) to provide interactive, conversational movie recommendations based on the IMDb dataset.

### Business Objectives
1.  **Interactive Search**: Enable natural language queries for intuitive movie discovery.
2.  **Personalized Recommendations**: Provide context-aware suggestions based on user preference.
3.  **Enhanced User Experience (UX)**: Deliver instant, accurate responses including ratings, metascores, and reviews.
4.  **Automation**: Reduce the manual effort required to filter through large movie databases.

---

## ‚öôÔ∏è Technical Stack

### Core Frameworks
* **Orchestration**: LangChain
* **LLM**: OpenAI GPT (gpt-3.5-turbo / gpt-4)
* **Embeddings**: OpenAI `text-embedding-ada-002`

### Data Infrastructure
* **Vector Store**: FAISS (Facebook AI Similarity Search)
* **Data Source**: IMDb Dataset (3,000+ curated movie entries)

### Interface & Deployment
* **User Interface**: Gradio
* **Environment**: Python

---

## Dataset Overview

**Source**: [IMDb Dataset Access Link](https://drive.google.com/file/d/1faiO17l7NlmPny7J_-6iu4VihIKqR-pZ/view?usp=drive_link)

**Key Features**:
* **Metadata**: Title, Year, Duration, Certificate
* **Content**: Genre, Plot Description (used for embedding)
* **Talent**: Director, Cast
* **Metrics**: Rating, Metascore
* **Assets**: Poster URLs

---

## Project Goals

* ‚úÖ **RAG Pipeline**: Build an end-to-end Retrieval-Augmented Generation system.
* ‚úÖ **Agentic Architecture**: Implement a multi-agent setup for complex query handling.
* ‚úÖ **Robustness**: Handle edge cases and ambiguous queries gracefully.
* ‚úÖ **UI/UX**: Create an intuitive conversational interface using Gradio.
* ‚úÖ **Quality Assurance**: Achieve "Excellent" rating on all project rubric criteria.

---

## Table of Contents

1.  Setup & Dependencies
2.  Data Loading & Exploratory Data Analysis (EDA)
3.  Data Preprocessing & Embedding
4.  RAG Pipeline Construction (Vector Store)
5.  LLM & Chain Configuration
6.  Agentic AI Implementation
7.  Edge Case Handling Strategy
8.  User Interface Development
9.  Evaluation & Testing
10. Future Improvements

---

## Implementation Note: Architectural Decision

> **Legacy Chains vs. LCEL**

**Current Approach**:
I deliberately selected the explicit chain definition style (Legacy Chains) to ensure a granular understanding of the component architecture before abstracting to the newer syntax.

**Comparison**:

**1. Current Implementation (Explicit)**
```python
document_chain = create_stuff_documents_chain(llm, prompt)
retrieval_chain = create_retrieval_chain(retriever, document_chain)

# Section 1: Setup & Dependencies

## Install Dependencies
"""

# Commented out IPython magic to ensure Python compatibility.
# Install Dependencies
print("Cleaning up old versions...")
# %pip uninstall -y -q langchain langchain-community langchain-core langchain-openai langchain-text-splitters langchain-huggingface

print("Installing Google Generative ai...")
!pip install -q -U google-generativeai

print("Installing core LangChain framework...")
# %pip install -q "langchain<1.0" "langchain-community<1.0" "langchain-core<1.0" "langchain-openai<1.0" "langchain-text-splitters<1.0"

print("Installing vector store and embeddings...")
# %pip install -q faiss-cpu sentence-transformers

print("Installing OpenAI client...")
# %pip install -q openai

print("Installing UI framework...")
# %pip install -q gradio
!pip install nest_asyncio

print("Installing utilities...")
# %pip install -q tiktoken pandas numpy matplotlib seaborn rapidfuzz

print("Need to RESTART RUNTIME now, Do not forget!!")

print("‚úÖInstallation is done!")

"""## Import Libraries

"""

# --- 1. PYTHON STANDARD LIBRARY ---
import os
import time
import re
import string
import random
import logging
import traceback
import warnings
from functools import wraps
from typing import Optional, Tuple, List, Dict, Any, Callable

# I use 'warnings' to keep the notebook output clean for the demo.
warnings.filterwarnings('ignore')

# --- 2. DATA SCIENCE & UTILITIES ---
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from rapidfuzz import fuzz # Essential for fuzzy matching in the "Parrot Protocol"

# --- 3. LANGCHAIN CORE & LLM ---
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.documents import Document
from langchain.tools import tool

# --- 4. RAG PIPELINE COMPONENTS ---
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain

# --- 5. AGENT ORCHESTRATION ---
from langchain.agents import create_openai_tools_agent, AgentExecutor

# --- 6. UI & MULTIMODAL (GRADIO & VISION) ---
import gradio as gr
import google.generativeai as genai # For the vision capabilities
from PIL import Image
import nest_asyncio # Essential for running Gradio within Colab

# --- 7. ENVIRONMENT & AUTHENTICATION ---
from dotenv import load_dotenv
from google.colab import drive, userdata

# Initialize environment
load_dotenv()

print("‚úÖAll libraries imported successfully. Ready to build!")

"""# Section 1: Data Loading & EDA
## Data Loading Strategy

I started by mounting Google Drive to access the persistent storage. To speed up future runs, I implemented a caching mechanism: if a pickle file (`processed_movies.pkl`) exists, I load that directly. Otherwise, I load the raw CSV, perform the necessary column renaming to fix formatting issues (like removing spaces in column names), and prepare it for processing.

## Code - Mount Google Drive & Load Data
"""

# Mount Google Drive to access the dataset
drive.mount('/content/drive')

# Check for cached data to save time on subsequent runs
if os.path.exists('processed_movies.pkl'):
    df = pd.read_pickle('processed_movies.pkl')
    print("‚úÖ Loaded cached dataframe from pickle file.")
else:
    try:
        # Load the raw CSV
        file_path = '/content/drive/MyDrive/AIML/CASE_STUDY_2/IMDb_Dataset.csv'
        df = pd.read_csv(file_path)

        # Rename columns to be Python-friendly (remove spaces/special chars)
        column_mapping = {
            'IMDb Rating': 'Rating',
            'Star Cast': 'Cast',
            'Poster-src': 'Poster',
            'Duration (minutes)': 'Duration'
        }
        df.rename(columns=column_mapping, inplace=True)
        print(f"‚úÖ Successfully loaded raw data. Columns renamed: {df.columns.tolist()}")

    except FileNotFoundError:
        print(f"‚ùå Error: Could not find file at {file_path}. Please check the path.")
    except Exception as e:
        print(f"‚ùå An unexpected error occurred: {e}")

"""## Exploratory Data Analysis (EDA)

Before building the RAG system, I needed to understand the shape and quality of the data. I specifically looked for:
1.  **Class Imbalance:** Are the genres evenly distributed?
2.  **Data Quality:** How many missing values (Nulls) do I need to handle?
3.  **Distributions:** What is the average rating? (This helps calibrate the "Search Agent" later).

## 5: Code - Basic Data Exploration
   - df.head(), df.info(), df.describe()
   - Check for nulls
"""

# View & Understand the data
print(f"Shape: {df.shape[0]} rows √ó {df.shape[1]} columns\n")
print("First few rows:\n")
display(df.head())
print("Information about the data set:\n")
display(df.info())
print("Data Statistical Summary:\n")
display(df.describe())
print("Checking for nulls:\n")
display(df.isnull().sum())

"""## Code - EDA Visualizations
   - Genre distribution
   - Rating distribution
   - Year trends
"""

# Exploratory Data Analysis - Visualizations
# Now let's visualize the data
fig, axes = plt.subplots(2, 2, figsize=(16, 12))
fig.suptitle('IMDb Dataset Analysis', fontsize=16, fontweight='bold')

# 1. Genre distribution
ax1 = axes[0, 0]
genre_counts = df['Genre'].value_counts().head(20)
genre_counts.plot(kind='barh', ax=ax1, color=sns.color_palette("husl", 20))
ax1.set_title('Top 15 Genres')
ax1.set_xlabel('Number of Movies')
ax1.grid(axis='x', alpha=0.3)

# Add counts on bars
for i, v in enumerate(genre_counts.values):
    ax1.text(v + 5, i, str(v), va='center')

# 2. Rating distribution
ax2 = axes[0, 1]
ratings = df['Rating'].dropna()
ax2.hist(ratings, bins=30, color='skyblue', edgecolor='black', alpha=0.7)
ratings.plot(kind='kde', ax=ax2, color='red', linewidth=2, secondary_y=True)
ax2.set_title('Rating Distribution')
ax2.set_xlabel('IMDb Rating')
ax2.axvline(ratings.mean(), color='green', linestyle='--', linewidth=2)
ax2.axvline(ratings.median(), color='orange', linestyle='--', linewidth=2)

# 3. Movies per year
ax3 = axes[1, 0]
years = df['Year'].dropna()
years = years[(years >= 1900) & (years <= 2025)]
year_counts = years.value_counts().sort_index()
ax3.plot(year_counts.index, year_counts.values, color='darkblue', linewidth=2)
ax3.fill_between(year_counts.index, year_counts.values, alpha=0.3)
ax3.set_title('Movies Released Per Year')
ax3.set_xlabel('Year')
ax3.set_ylabel('Count')
ax3.grid(alpha=0.3)

# 4. Rating vs Year - does quality change over time?
ax4 = axes[1, 1]
plot_data = df[['Year', 'Rating']].dropna()
plot_data = plot_data[(plot_data['Year'] >= 1900) & (plot_data['Year'] <= 2025)]
ax4.scatter(plot_data['Year'], plot_data['Rating'], alpha=0.3, s=20, color='purple')

# Add trend line
z = np.polyfit(plot_data['Year'], plot_data['Rating'], 1)
p = np.poly1d(z)
ax4.plot(plot_data['Year'], p(plot_data['Year']), "r--", linewidth=2)
ax4.set_title('Rating vs Year')
ax4.set_xlabel('Year')
ax4.set_ylabel('IMDb Rating')
ax4.grid(alpha=0.3)

plt.tight_layout()
plt.show()

# Print some interesting findings
print("\nKey findings:")
print(f"- Most common genre: {df['Genre'].value_counts().index[0]} ({df['Genre'].value_counts().values[0]} movies)")
print(f"- Average rating: {ratings.mean():.2f}")
print(f"- Movies from 2010+: {(years >= 2010).sum()} ({(years >= 2010).sum()/len(years)*100:.0f}%)")
print(f"- Highly rated movies (>8.0): {(ratings > 8.0).sum()}")

"""## EDA Summary

Dataset: 3,173 movies spanning 1917-2025

**Key observations:**
- Genre heavy on Biography (21%), Action (18%), Drama (13%)
- Peak production year: 2018 (147 movies)
- Ratings average 7.2 - mostly higher-quality films
- MetaScore missing for ~500 movies, but IMDb ratings complete

Fixed column names first - spaces in column names break pandas dot notation.

# Section 2: Data Preprocessing & Feature Engineering

My goal here was to prepare the textual data for the vector store. Since LLMs work best with rich context, I couldn't just use the movie titles.

**My Approach:**
1.  **Imputation:**

I filled missing numeric values (Year, Duration) with the median to maintain statistical validity, and replaced missing text with "Unknown."

2.  **Context Creation:**

I engineered a new feature called `Description`. This combines the Title, Genre, Rating, Director, and Cast into a single natural language paragraph. This "Rich Text" representation allows the RAG system to find movies based on *any* of these attributes, not just the plot.

## Code - Handle Missing Values
"""

# See what's missing in the dataset

missing = df.isnull().sum()
missing = missing[missing > 0]

if len(missing) > 0:
    print("Missing values found:")
    for col, count in missing.items():
        pct = (count / len(df)) * 100
        print(f"  {col}: {count} ({pct:.1f}%)")
else:
    print("No missing values - nice!")

# Handle the gaps
# MetaScore has a bunch missing but that's fine, we have IMDb ratings
# For text stuff, just fill with 'Unknown' so it doesn't break later

text_cols = ['Director', 'Cast', 'Genre', 'Certificates']
for col in text_cols:
    if col in df.columns and df[col].isnull().any():
        df[col].fillna('Unknown', inplace=True)
        print(f"Filled missing {col} with 'Unknown'")

# Year and Duration - use median if any are missing
if df['Year'].isnull().any():
    df['Year'].fillna(df['Year'].median(), inplace=True)
    print("Filled missing years with median")

if df['Duration'].isnull().any():
    df['Duration'].fillna(df['Duration'].median(), inplace=True)
    print("Filled missing durations with median")

print(f"\nDone. Remaining nulls: {df.isnull().sum().sum()}")

# I did not need all these lines for this project, after the first two lines showed that we don't have missing data. but I have these to be production ready and won't break on new or different data.

"""## Code - Create Movie Description"""

# Create the "Rich Context" for the RAG retriever
def make_description(row):
    """
    Combines structured tabular data into a natural language string.
    This string will be embedded to allow semantic search across all fields.
    """
    # Base description
    desc = (
        f"Title: {row['Title']}\n"
        f"Year: {int(row['Year'])}\n"  # Ensure Year is an integer
        f"Genre: {row['Genre']}\n"
        f"Rating: {row['Rating']}/10\n"
        f"Director: {row['Director']}\n"
        f"Cast: {row['Cast']}\n"
        f"Duration: {int(row['Duration'])} minutes\n"
        f"Certificate: {row['Certificates']}"
    )

    # Add MetaScore only if it exists (provides extra context for the 'Comparison Agent')
    if pd.notna(row['MetaScore']):
        desc += f"\nMetaScore: {row['MetaScore']}/100"

    return desc

# Apply the function
df['Description'] = df.apply(make_description, axis=1)

# Persist the processed data
df.to_pickle('processed_movies.pkl')
print(f"‚úÖ Generated descriptions for {len(df)} movies and saved to pickle.")

"""## Code - Display Sample Descriptions"""

for i in df.sample(3).index:
    print(f"\n{df.loc[i, 'Title']}:")
    print(df.loc[i, 'Description'])
    print("-" * 60)

"""# Section 3: Building the RAG Pipeline

My next step was to build the retrieval system. I needed to convert the natural language descriptions I created in Section 2 into numerical vectors (embeddings) that represent the semantic meaning of each movie.

## Text Splitting Strategy
I chose a chunk size of **250 characters** with a **20-character overlap**. Since each movie description is a concise summary, smaller chunks allow the model to pinpoint specific details (like a director's name or a plot point) without getting lost in unrelated text.
"""

# 1. Document Creation
# I convert the DataFrame rows into LangChain Document objects, using the title as metadata.
docs = [
    Document(page_content=desc, metadata={'title': title})
    for desc, title in zip(df['Description'], df['Title'])
]
print(f"Created {len(docs)} documents.")

# 2. Text Splitting
# I used RecursiveCharacterTextSplitter to respect sentence boundaries.
splitter = RecursiveCharacterTextSplitter(
    chunk_size=250,
    chunk_overlap=20,
    length_function=len
)
chunks = splitter.split_documents(docs)

print(f"Split {len(docs)} movies into {len(chunks)} searchable chunks.")

"""## Security & Authentication
I implemented a secure API key retrieval system using Colab's `userdata` to avoid hardcoding secrets in the notebook.
"""

try:
    # I securely fetch the key from Colab secrets
    api_key = userdata.get('OPENAI_API_KEY')
    os.environ['OPENAI_API_KEY'] = api_key
    print(f"‚úÖ Key loaded successfully: {api_key[:8]}...")
except KeyError:
    print("‚ùå Key not found! Please add 'OPENAI_API_KEY' to Colab secrets.")
    raise

"""## Embedding Model Selection
I selected `OpenAIEmbeddings` (ada-002) for its state-of-the-art performance in semantic similarity tasks. This ensures that queries like "scary movie in space" correctly map to "Alien" even if the exact keywords don't match.
"""

# Initialize the embedding model
embeddings = OpenAIEmbeddings()
print("‚úÖ Embedding model initialized.")

"""## Vector Database (FAISS) with Caching
I implemented **FAISS (Facebook AI Similarity Search)** for efficient local clustering. To optimize performance and cost, I added a caching logic: if a local index exists, I load it; otherwise, I generate new embeddings and save them.

### Delete saved store if you:

 - Change movie descriptions
 - Add new movies
 - Change chunk size/overlap
"""

# In case of # Force recreate
# !rm -rf movie_vectorstore

# Check for existing vector store to save time/cost
if os.path.exists("movie_vectorstore"):
    print("Loading existing FAISS vector store...")
    vectorstore = FAISS.load_local(
        "movie_vectorstore",
        embeddings,
        allow_dangerous_deserialization=True
    )
    print(f"‚úÖ Loaded {vectorstore.index.ntotal} vectors from cache.")
else:
    print("Creating new FAISS vector store...")
    vectorstore = FAISS.from_documents(chunks, embeddings)
    vectorstore.save_local("movie_vectorstore")
    print(f"‚úÖ Created and saved {len(chunks)} vectors.")

"""## Retrieval Verification
Before building the LLM chain, I ran a quick similarity search to verify the vector store returns relevant results.
"""

# Test Query
query = "action movie from the 90s"
results = vectorstore.similarity_search(query, k=3)

print(f"Test Query: '{query}'\n")
for i, doc in enumerate(results, 1):
    print(f"{i}. {doc.metadata['title']}")
    print(f"   Context: {doc.page_content[:80]}...\n")

"""# Section 4: LLM & Chain

I will focus on the MMR (Maximal Marginal Relevance) choice.

## LLM Configuration
I initialized `GPT-3.5-Turbo` with a temperature of **0.7**. This specific setting strikes a balance between factual accuracy (sticking to the retrieved context) and conversational creativity (sounding charismatic).
"""

# Initialize the primary chat model
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)
print("‚úÖ LLM initialized.")

"""## System Prompt Design
I designed a system prompt that explicitly instructs the AI to act as a "Movie Expert." Crucially, I added a directive to handle cases where the answer isn't in the database, preventing hallucinations.
"""

# Define the RAG prompt template
prompt = ChatPromptTemplate.from_template("""
You are a knowledgeable and charismatic movie expert.
Use the following retrieved movie data to answer the user's request.

<context>
{context}
</context>

User Request: {input}

**Instructions:**
1. Recommend movies based strictly on the context provided.
2. If the context doesn't contain the exact answer, use your general knowledge but mention that it's outside your local database.
3. Be concise but friendly. Include the rating and year in your recommendation.
""")

# Create the document chain
doc_chain = create_stuff_documents_chain(llm, prompt)
print("‚úÖ Document chain created.")

"""## Advanced Retrieval Strategy (MMR)
Instead of standard similarity search, I utilized **Maximal Marginal Relevance (MMR)**.
* **Why?**

Standard search often returns redundant results (e.g., *Star Wars 1, 2, 3*). MMR fetches a larger pool of candidates (k=20) and filters them to ensure the final top 5 results are diverse, providing a broader range of recommendations.
"""

# Configure the retriever with MMR
retriever = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 5, "fetch_k": 20} # Fetch 20, return top 5 diverse
)
print("‚úÖ MMR Retriever configured.")

"""## Final Chain Assembly
I combined the MMR retriever with the document chain to create the final Retrieval-Augmented Generation pipeline.
"""

# Build the full RAG pipeline
rag_chain = create_retrieval_chain(retriever, doc_chain)
print("‚úÖ RAG Chain successfully assembled.")

"""## End-to-End Test
I verified the full pipeline with a specific query to ensure the LLM correctly synthesizes the retrieved context.
"""

# Test the full chain
response = rag_chain.invoke({
    "input": "Recommend a highly rated drama from the late 70s"
})

print(f"User: {response['input']}")
print(f"Bot: {response['answer']}")

# Optional: Test the functionality using a Gradio UI (intermediate check)
## I prefer to continue with the logic and do the UI at section 7, Now that it is optional.

"""# Section 5: Agentic AI Architecture

A single RAG chain is insufficient for complex interactions that require maintaining state (like a game) or performing specific calculations. To solve this, I implemented an **Orchestrator-Worker** pattern with 7 specialized agents.

## 5.1 The Agent Roster

**1. Search Agent**
* Finds movies by filters (genre, year, rating).
* Handles natural language queries like "90s action movies" or "comedies rated 8+".
* Returns a list of matching movies with metadata.

**2. Recommendation Agent**
* Suggests movies similar to a specific title provided by the user.
* Uses vector similarity (Cosine/Euclidean distance via FAISS) to find thematic matches.
* Considers plot, genre, and director style in its recommendations.

**3. Analytics Agent**
* Answers statistical questions that require aggregation.
* Calculates averages (e.g., "average rating of 2024 movies") and counts.
* Provides data-driven insights rather than just retrieving text.

**4. Person Agent**
* A dedicated search tool for Directors and Actors.
* Searches cast and crew information to return filmographies.
* Useful for "What movies did Nolan direct?" style queries.

**5. Comparison Agent**
* Analyzes the disparity between User Ratings (IMDb) and Critic Scores (MetaScore).
* Identifies "Cult Classics" (Low Critic/High User) or "Critical Darlings" (High Critic/Low User).

**6. Trending Agent**
* Fetches recent high-performing movies (e.g., 2023-2025).
* Filters results by high ratings to surface "What's hot right now".

**7. Quiz Agent ("The Gamemaster")**
* Generates interactive trivia questions (Actor, Director, or Year challenges).
* **Crucial Feature:** Implements the "Parrot Protocol" for state management.

The **Orchestrator (LLM)** analyzes the user's intent and routes the query to the correct worker.

### Advanced Features

**Voice-Based Search:**
- Web Speech API integration (free)
- Speak your queries naturally
- Browser-based speech recognition

**Image Input (Gemini Vision):**
- Upload movie posters
- Identify movies from images
- Get recommendations based on visual style

### How It Works

The LLM acts as the orchestrator using ReAct prompting:
1. User asks a question (text, voice, or image)
2. LLM analyzes what type of query it is
3. LLM chooses the right tool(s) to use
4. Agent executes and returns results
5. LLM formats final response

**Example:**
```
User: "What are the highest rated comedies from the 2000s?"
    ‚Üì
LLM thinks: "This needs search + analytics"
    ‚Üì
Calls Search Agent (comedies, 2000s)
    ‚Üì
Calls Analytics Agent (sort by rating)
    ‚Üì
Returns: "Here are the top-rated comedies from 2000-2009..."
```

This modular approach makes the system more maintainable and accurate.

## State Management: The "Parrot Protocol"

One of the biggest challenges in building an LLM-based game is **State Management**. Standard LLM agents are stateless; they don't natively "remember" the correct answer to a question they just asked, often leading to hallucinations where the bot accepts a wrong answer or generates a new question immediately.

**My Solution:**
I devised a technique I call the **"Parrot Protocol"**.
1.  **Embedding State:** When the Quiz Agent generates a question, it appends a hidden key to the output: `||INTERNAL_ANSWER_KEY: [Correct Answer]||`.
2.  **State Detection:** The Orchestrator's System Prompt is trained to scan conversation history for this specific pattern.
3.  **Strict Grading:** If the key is detected, the Orchestrator **locks out all tools**. It enters a "Grader Mode" where its *only* job is to compare the user's input string against the hidden key.
4.  **Result:** This forces the LLM to act as a strict logic gate ("Parroting" the truth) rather than a creative writer, ensuring 100% accurate grading without hallucinations.

## Helper methods
"""

# HELPER FUNCTIONS - Intent Parsing & Entity Extraction

def parse_year_range(query_lower):
    """Extract year range (decade or specific year) from query string."""
    # Handle decades (e.g., "90s", "1990s")
    decade_match = re.search(r'\b(\d{2,4})\s?\'?s\b', query_lower)
    if decade_match:
        digits = decade_match.group(1)
        start_year = int("19" + digits) if len(digits) == 2 else int(digits)
        return (start_year, start_year + 9)

    # Handle specific years (e.g., "2023")
    year_match = re.search(r'\b(19\d{2}|20\d{2})\b', query_lower)
    if year_match:
        specific_year = int(year_match.group(1))
        return (specific_year, specific_year)

    return None

def parse_genre(query_lower):
    """
    Extract genre using Exact Match first, then Smart Fuzzy matching for typos.
    """
    genres = ['action', 'adventure', 'animation', 'biography', 'comedy', 'crime',
              'documentary', 'drama', 'fantasy', 'family', 'history', 'horror',
              'musical', 'mystery', 'romance', 'sci-fi', 'reality-tv']

    # Handle Sci-Fi variations explicitly
    if 'scifi' in query_lower or ('sci' in query_lower and 'fi' in query_lower):
        return 'sci-fi'

    # 1. Check for Exact Match
    for genre in genres:
        # Pad with spaces to avoid partial word matches
        if f" {genre} " in f" {query_lower} ":
            return genre

    # 2. Check for Fuzzy Match (Typo Tolerance)
    ignore_words = ['story', 'movie', 'film', 'highest', 'rated', 'show', 'watch', 'cinema']
    words = query_lower.split()

    for word in words:
        if word in ignore_words:
            continue

        # Score word against all genres
        scores = []
        for genre in genres:
            score = fuzz.ratio(genre, word)
            scores.append((genre, score))

        # Get best match
        scores.sort(key=lambda x: x[1], reverse=True)
        best_genre, best_score = scores[0]
        _, second_score = scores[1]

        # Smart Match Logic: High score OR decent score with clear separation
        if best_score >= 80:
            return best_genre
        if best_score >= 60 and (best_score - second_score) >= 10:
            return best_genre

    return None

def is_highest_rated_query(query_lower):
    """Detect if query asks for highest/best/top rated movies."""
    keywords = ["highest rated", "best rated", "top rated", "best movies",
                "top movies", "highest ranking", "top ranking"]
    for keyword in keywords:
        if fuzz.partial_ratio(keyword, query_lower) > 80:
            return True
    return False

def is_average_rating_query(query_lower):
    """Detect if query asks for average ratings."""
    keywords = ["average rating", "avg rating", "mean rating", "average score"]
    for keyword in keywords:
        if fuzz.partial_ratio(keyword, query_lower) > 80:
            return True
    return False

def is_count_query(query_lower):
    """Detect if query asks for counts/numbers."""
    keywords = ["how many", "count", "number of", "total movies", "total number"]
    for keyword in keywords:
        if fuzz.partial_ratio(keyword, query_lower) > 80:
            return True
    return False

def is_high_rating_filter(query_lower):
    """Detect if query wants high-rated movies (for search filter)."""
    keywords = ["high rated", "highly rated", "best", "top quality"]
    for keyword in keywords:
        if fuzz.partial_ratio(keyword, query_lower) > 75:
            return True
    return False

def parse_rating_filter(query_lower):
    """Extract specific numeric rating filter from query string."""
    # Check for general "high rating" keywords first
    if is_high_rating_filter(query_lower):
        return 7.5
    # Check for specific numbers (e.g., "rated above 8")
    elif "rating above" in query_lower or "rating over" in query_lower:
        rating_match = re.findall(r'(?:above|over) (\d+(?:\.\d+)?)', query_lower)
        if rating_match:
            return float(rating_match[0])
    return None

def clean_mashed_names(text):
    """
    Splits mashed names like 'LeonardoDiCaprio' into 'Leonardo, DiCaprio'.
    Useful for cleaning up raw dataset strings.
    """
    # Regex: Look behind for a-z, Look ahead for A-Z
    cleaned = re.sub(r'(?<=[a-z])(?=[A-Z])', ', ', text)
    return cleaned

print("‚úÖ Helper functions loaded.")

"""## Search & Recommendation Agents

###  Search Agent
- Finds movies by filters (genre, year, rating)
- Handles decade queries (90s, 2010s)
- Returns matching movies

### Recommendation Agent
- Suggests movies similar to a given title
- Uses vector similarity for matching
- Considers genre and style
"""

# --- Tool 1: The Search Agent ---
@tool
def search_movies(query: str) -> str:
    """
    Finds movies based on user criteria like genre, year, or rating.
    Use this when user wants to search or find movies matching specific filters.
    """
    query_lower = query.lower()

    # Parse intent using our helper functions
    year_range = parse_year_range(query_lower)
    genre_filter = parse_genre(query_lower)
    rating_filter = parse_rating_filter(query_lower)

    # 1. Primary Method: Vector Search (Semantic)
    results = vectorstore.similarity_search(query, k=30)

    # 2. Filtering Logic
    filtered = []
    seen_titles = set()

    for doc in results:
        title = doc.metadata.get('title', 'Unknown')
        if title in seen_titles: continue

        # Check Year
        if year_range:
            doc_year_match = re.search(r'Year: (\d{4})', doc.page_content)
            if doc_year_match:
                movie_year = int(doc_year_match.group(1))
                if not (year_range[0] <= movie_year <= year_range[1]):
                    continue

        # Check Rating
        if rating_filter:
            rating_match = re.search(r'Rating: ([\d.]+)/10', doc.page_content)
            if rating_match:
                movie_rating = float(rating_match.group(1))
                if movie_rating < rating_filter:
                    continue

        # Check Genre
        if genre_filter:
            if genre_filter not in doc.page_content.lower():
                continue

        seen_titles.add(title)
        filtered.append(doc)
        if len(filtered) >= 5: break

    # 3. Fallback Method: Pandas Filter
    # If vector search didn't yield enough results, try strict filtering on the DataFrame
    if len(filtered) < 5 and (year_range or rating_filter or genre_filter):
        df_filtered = df.copy()

        if year_range:
            df_filtered = df_filtered[(df_filtered['Year'] >= year_range[0]) &
                                       (df_filtered['Year'] <= year_range[1])]
        if rating_filter:
            df_filtered = df_filtered[df_filtered['Rating'] >= rating_filter]
        if genre_filter:
            df_filtered = df_filtered[df_filtered['Genre'].str.contains(genre_filter, case=False, na=False)]

        # Prioritize high rated ones
        df_filtered = df_filtered.sort_values('Rating', ascending=False)

        for idx, row in df_filtered.iterrows():
            if row['Title'] not in seen_titles and len(filtered) < 5:
                # Create a synthetic document for consistency
                fake_doc = Document(
                    page_content=f"Title: {row['Title']}\nYear: {int(row['Year'])}\nGenre: {row['Genre']}\nRating: {row['Rating']}/10",
                    metadata={'title': row['Title']}
                )
                filtered.append(fake_doc)
                seen_titles.add(row['Title'])

    # Format Output
    if not filtered:
        return "No movies found matching those criteria. Try broadening your search."

    output = "Here's what I found:\n\n"
    for i, doc in enumerate(filtered, 1):
        # Extract cleaner details for display
        lines = doc.page_content.split('\n')[:4]
        info = ' | '.join([line.split(': ')[1] for line in lines if ': ' in line])
        output += f"{i}. {info}\n"

    return output

# --- Tool 2: The Recommendation Agent ---
@tool
def recommend_similar_movies(movie_title: str) -> str:
    """
    Finds movies similar to a given movie title using vector similarity.
    """
    # 1. Find the reference movie
    ref_results = vectorstore.similarity_search(f"Title: {movie_title}", k=1)

    # 2. Fuzzy Match Validation
    # If exact title isn't found, check if we found a close match
    if not ref_results or fuzz.ratio(ref_results[0].metadata['title'].lower(), movie_title.lower()) < 70:
        # Try finding it in the dataframe as a fallback
        best_match = None
        best_score = 0
        for title in df['Title']:
            score = fuzz.ratio(title.lower(), movie_title.lower())
            if score > best_score:
                best_score = score
                best_match = title

        if best_score > 70:
            ref_results = vectorstore.similarity_search(f"Title: {best_match}", k=1)
            suggestion = f"\n(Note: Did you mean '{best_match}'?)\n"
        else:
            return f"Could not find '{movie_title}' in the database. Try checking the spelling."
    else:
        suggestion = ""

    # 3. Find Recommendations based on the reference content
    ref_content = ref_results[0].page_content
    similar = vectorstore.similarity_search(ref_content, k=10)

    results = []
    seen_titles = set()

    for doc in similar:
        title = doc.metadata.get('title', 'Unknown')
        # Filter out the movie itself and duplicates
        if title == ref_results[0].metadata['title'] or title in seen_titles:
            continue
        seen_titles.add(title)
        results.append(doc)
        if len(results) >= 5: break

    # Format Output
    output = suggestion + f"Movies similar to '{ref_results[0].metadata['title']}':\n\n"
    for i, doc in enumerate(results, 1):
        lines = doc.page_content.split('\n')[:4]
        info = ' | '.join([line.split(': ')[1] for line in lines if ': ' in line])
        output += f"{i}. {info}\n"

    return output

print("‚úÖ Search & Recommendation Agents initialized.")

"""## Analytics & Person Agents

### Analytics Agent
- Answers statistical questions
- Highest rated, averages, counts
- Provides data-driven insights
### Person Agent
- Finds movies by actor or director
- Searches cast and crew info
- Returns filmography results
"""

# --- Tool 3: Analytics Agent ---
@tool
def get_movie_statistics(query: str) -> str:
    """
    Answers statistical questions: Highest rated, averages, or counts.
    """
    query_lower = query.lower()

    # Parse filters
    year_range = parse_year_range(query_lower)
    genre_filter = parse_genre(query_lower)

    # Create a filtered view of the data
    filtered_df = df.drop_duplicates(subset=['Title']).copy()

    if genre_filter:
        filtered_df = filtered_df[filtered_df['Genre'].str.contains(genre_filter, case=False, na=False)]
    if year_range:
        filtered_df = filtered_df[(filtered_df['Year'] >= year_range[0]) &
                                   (filtered_df['Year'] <= year_range[1])]

    # Branch Logic based on Intent Helpers
    if is_highest_rated_query(query_lower):
        top_movies = filtered_df.nlargest(5, 'Rating')[['Title', 'Rating', 'Year', 'Genre']]

        genre_desc = f"{genre_filter} " if genre_filter else ""
        year_desc = f"from {year_range[0]}-{year_range[1]} " if year_range else ""
        result = f"Highest rated {genre_desc}movies {year_desc}:\n\n"

        for idx, row in top_movies.iterrows():
            result += f"- {row['Title']} ({int(row['Year'])}): {row['Rating']}/10\n"
        return result

    elif is_average_rating_query(query_lower):
        avg_rating = filtered_df['Rating'].mean()
        count = len(filtered_df)

        genre_desc = f"{genre_filter} " if genre_filter else ""
        year_desc = f"from {year_range[0]}-{year_range[1]} " if year_range else ""
        return f"Average rating for {genre_desc}movies {year_desc}: {avg_rating:.2f}/10 (based on {count} movies)"

    elif is_count_query(query_lower):
        count = len(filtered_df)

        genre_desc = f"{genre_filter} " if genre_filter else ""
        year_desc = f"from {year_range[0]}-{year_range[1]} " if year_range else ""
        return f"Found {count} {genre_desc}movies {year_desc}in the database."

    else:
        return "I can answer questions about highest rated movies, average ratings, or counts. Try rephrasing."

# --- Tool 4: Person Agent ---
@tool
def search_by_person(name: str) -> str:
    """
    Search for movies associated with a specific person (Actor/Director).
    """
    person_name = name.lower().strip()
    if len(person_name) < 2: return "Please provide a valid name."

    # Use Vector Search for efficiency
    results = vectorstore.similarity_search(person_name, k=15)

    filtered_results = []
    seen_titles = set()

    for doc in results:
        title = doc.metadata.get('title', 'Unknown')

        # Double check the name actually appears in the text
        if person_name in doc.page_content.lower() and title not in seen_titles:
            lines = doc.page_content.split('\n')
            # Extract key info
            info_lines = [l for l in lines if any(k in l for k in ['Year', 'Genre', 'Rating'])]
            info_text = ' | '.join(info_lines)

            filtered_results.append(f"- {title}: {info_text}")
            seen_titles.add(title)

        if len(filtered_results) >= 5: break

    if not filtered_results:
        return f"I couldn't find any movies associated with '{name}'."

    return f"Here are movies associated with {name.title()}:\n\n" + "\n".join(filtered_results)

print("‚úÖ Analytics & Person Agents initialized.")

"""## Comparison, Trending, Quiz Agents

### Comparison Agent (Review Analysis)
- Analyzes IMDb vs MetaScore ratings
- Compares user vs critic opinions
- Identifies rating gaps and disagreements
### Trending Agent
- Surfaces recent popular movies (2020+)
- Identifies what's hot right now
- Filters by high ratings and recency
### Quiz Agent (Interactive Feature)
- Generates movie trivia questions
- Creates interactive quizzes
- Tests movie knowledge with feedback
"""

# --- Tool 5: Comparison Agent ---
@tool
def compare_ratings(movie_title: str) -> str:
    """
    Compare IMDb (User) rating vs MetaScore (Critic) rating for a movie.
    """
    # 1. Find the movie
    matches = df[df['Title'].str.contains(movie_title, case=False, na=False)]

    if matches.empty:
        # Fuzzy Fallback
        best_match = None
        best_score = 0
        for title in df['Title']:
            score = fuzz.ratio(title.lower(), movie_title.lower())
            if score > best_score:
                best_score = score
                best_match = title

        if best_score > 80:
            matches = df[df['Title'] == best_match]
        else:
            return f"I couldn't find rating data for '{movie_title}'."

    # 2. Extract Ratings
    movie = matches.iloc[0]
    imdb = movie['Rating']
    metascore = movie['MetaScore']

    if pd.isna(metascore) or metascore == 'Unknown':
        return f"'{movie['Title']}' has an IMDb rating of {imdb}/10, but no MetaScore is available."

    # 3. Analyze Gap
    meta_norm = metascore / 10
    diff = imdb - meta_norm

    response = f"**{movie['Title']} ({int(movie['Year'])})**\n"
    response += f"üé¨ IMDb (Users): {imdb}/10\n"
    response += f"üì∞ MetaScore (Critics): {metascore}/100 (approx {meta_norm:.1f}/10)\n\n"

    if abs(diff) < 1.0:
        response += "‚úÖ **Consensus:** Users and critics agree on this movie."
    elif diff > 0:
        response += f"üìâ **Underrated by Critics:** Users liked it {diff:.1f} points more than critics."
    else:
        response += f"üìà **Critical Darling:** Critics liked it {abs(diff):.1f} points more than users."

    return response

# --- Tool 6: Trending Agent ---
@tool
def get_trending_movies(query: str = "") -> str:
    """
    Find trending movies (recent highly-rated releases, default 2023-2025).
    """
    query_lower = query.lower()

    year_range = parse_year_range(query_lower)
    if not year_range: year_range = (2023, 2025)

    genre_filter = parse_genre(query_lower)
    rating_filter = parse_rating_filter(query_lower)
    if not rating_filter: rating_filter = 7.0 # Default to good movies

    # Filter
    recent = df.drop_duplicates(subset=['Title']).copy()
    recent = recent[(recent['Year'] >= year_range[0]) & (recent['Year'] <= year_range[1])]
    recent = recent[recent['Rating'] >= rating_filter]

    if genre_filter:
        recent = recent[recent['Genre'].str.contains(genre_filter, case=False, na=False)]

    if recent.empty:
        return f"No trending movies found matching those criteria."

    # Sort
    trending = recent.sort_values(by='Rating', ascending=False).head(5)

    response = f"üî• Trending Movies ({year_range[0]}-{year_range[1]}):\n\n"
    for i, (_, row) in enumerate(trending.iterrows(), 1):
        response += f"{i}. {row['Title']} ({int(row['Year'])}) | {row['Genre']} | ‚≠ê {row['Rating']}\n"

    return response

# --- Tool 7: Quiz Agent ---
@tool
def generate_movie_quiz(query: str) -> str:
    """
    Generates a movie trivia question.
    """
    query_lower = query.lower()

    # Determine Quiz Mode
    modes = ['actor_challenge', 'director_challenge', 'movie_fact']
    if 'actor' in query_lower or 'star' in query_lower: mode = 'actor_challenge'
    elif 'director' in query_lower: mode = 'director_challenge'
    elif 'fact' in query_lower: mode = 'movie_fact'
    else: mode = random.choice(modes)

    # Create a pool of good movies (Recent & Good)
    year_range = parse_year_range(query_lower) or (2010, 2025)
    quiz_pool = df[(df['Year'] >= year_range[0]) & (df['Year'] <= year_range[1])]
    quiz_pool = quiz_pool[quiz_pool['Rating'] >= 7.0] # Only good movies

    if len(quiz_pool) < 5: return "Not enough popular movies found for a quiz."

    response = ""

    if mode == 'actor_challenge':
        # Clean up cast names
        quiz_pool['Cast'] = quiz_pool['Cast'].fillna("").astype(str)
        cleaned_series = quiz_pool['Cast'].apply(clean_mashed_names)

        # Pick a popular actor from this subset
        all_cast = cleaned_series.str.split(',').explode().str.strip()
        all_cast = all_cast[all_cast.str.len() > 1]
        top_actors = all_cast.value_counts().head(20).index.tolist()

        if not top_actors: return "Not enough data for actor quiz."
        target_actor = random.choice(top_actors)

        # Get answers
        answers = quiz_pool[quiz_pool['Cast'].str.contains(target_actor, regex=False)]['Title'].tolist()

        response = f"üé¨ **GAME: The Actor Challenge**\n"
        response += f"‚ùì Name a movie starring **{target_actor}** released between {year_range[0]}-{year_range[1]}."
        response += f"\n\n||INTERNAL_ANSWER_KEY: {', '.join(answers)}||"

    elif mode == 'director_challenge':
        target_director = random.choice(quiz_pool['Director'].unique())
        answers = quiz_pool[quiz_pool['Director'] == target_director]['Title'].tolist()

        response = f"üé¨ **GAME: The Director Challenge**\n"
        response += f"‚ùì Name a movie directed by **{target_director}** ({year_range[0]}-{year_range[1]})."
        response += f"\n\n||INTERNAL_ANSWER_KEY: {', '.join(answers)}||"

    else: # Fact Mode
        movie = quiz_pool.sample(1).iloc[0]
        if random.random() > 0.5:
            response = f"üé¨ **GAME: Movie Facts**\n‚ùì Who directed **'{movie['Title']}'**?"
            response += f"\n\n||INTERNAL_ANSWER_KEY: {movie['Director']}||"
        else:
            response = f"üé¨ **GAME: Movie Facts**\n‚ùì Name the main star of **'{movie['Title']}'**."
            clean_cast = clean_mashed_names(str(movie['Cast']))
            response += f"\n\n||INTERNAL_ANSWER_KEY: {clean_cast}||"

    return response

print("‚úÖ Comparison, Trending & Quiz Agents initialized.")

"""#### Tests


"""

# Test the Safety Logic
test_queries = [
    "tell me a story",               # Should be None (ignored 'story')
    "highest rated comdies",         # Should be 'comedy' (caught typo)
    "best history movies",           # Should be 'history'
    "show me a movie"                # Should be None
]

print("Testing Safe Genre Detection:")
for q in test_queries:
    print(f"'{q}' -> {parse_genre(q)}")

print(f"Exact match check: 'I want a comedy movie' -> {parse_genre('i want a comedy movie')}")
print(f"Typo match check:  'I want a comdies movie' -> {parse_genre('i want a comdies movie')}")

print(f"Exact match: {parse_genre('i want a comedy movie')}")  # Should be 'comedy'
print(f"Typo match:  {parse_genre('highest rated comdies')}")   # Should be 'comedy'
print(f"False pos:   {parse_genre('tell me a story')}")         # Should be None

# Correct testing (Simulating what the Agent will do)
print("Testing Agent inputs:")
print(search_by_person("Christopher Nolan"))   # Agent extracts "Christopher Nolan"
print("-" * 50)
print(search_by_person("Brad Pitt"))           # Agent extracts "Brad Pitt"

# Create a temporary column for the difference (User - Critic)
# Critic scores are /100, so we divide by 10 to match User scores /10
df['Diff'] = df['Rating'] - (df['MetaScore'] / 10)

# Sort by the biggest disagreement (absolute difference)
# We only look at movies that actually HAVE a MetaScore
disagreement = df[df['MetaScore'].notna()].copy()
disagreement['Abs_Diff'] = disagreement['Diff'].abs()

# Show top 5 "User Favorites" (Users liked it more)
print("--- USERS LOVED THESE (Critics didn't) ---")
user_favs = disagreement[disagreement['Diff'] > 0].sort_values('Abs_Diff', ascending=False).head(5)
for i, row in user_favs.iterrows():
    print(f"{row['Title']} (Diff: {row['Diff']:.1f})")

print("\n--- CRITICS LOVED THESE (Users didn't) ---")
# Show top 5 "Critic Favorites" (Critics liked it more)
critic_favs = disagreement[disagreement['Diff'] < 0].sort_values('Abs_Diff', ascending=False).head(5)
for i, row in critic_favs.iterrows():
    print(f"{row['Title']} (Diff: {row['Diff']:.1f})")

# Test Comparison Agent
print(compare_ratings("Modigliani"))  # Usually high agreement
print("-" * 50)
print(compare_ratings("Mr. Turner"))            # usually Critics hate it, Fans love it

print("--- TEST: Random Quiz ---")
print(generate_movie_quiz(""))

print("\n--- TEST: Actor Quiz ---")
print(generate_movie_quiz("give me an actor challenge"))

"""## Create Orchestrator Logic

### ORCHESTRATOR SETUP
"""

# Define the orchestrator logic to run the agents appropriately

# --- The Orchestrator Setup ---


tools = [
    search_movies,
    recommend_similar_movies,
    get_movie_statistics,
    search_by_person,
    compare_ratings,
    get_trending_movies,
    generate_movie_quiz
]

prompt = ChatPromptTemplate.from_messages([
    ("system", """You are a charismatic and knowledgeable movie expert AI.

    # üö® PRIORITY 1: TRIVIA GAME PROTOCOL (STRICT)

    **STEP A: DETECT STATE**
    - Look at the `chat_history` (the very last AI message).
    - **QUESTION:** Does it contain `||INTERNAL_ANSWER_KEY:`?

    **STEP B: IF KEY FOUND (GAME IS ACTIVE):**
    - **‚õî STOP! DO NOT CALL TOOLS.**
    - **INPUT:** User's text.
    - **TRUTH:** The text inside the previous `||...||`.
    - **ACTION:** Compare Input vs. TRUTH blindly.
       - If matches TRUTH -> Say "Correct! The answer was [TRUTH]."
       - If mismatch -> Say "Wrong! The answer was [TRUTH]."
    - **ENDING:** Ask "Ready for the next one?"

    **STEP C: STARTING A GAME:**
    - If user says "play", "quiz", "shoot", "next", "hit me" -> **CALL `generate_movie_quiz`**.

    # üé¨ PRIORITY 2: GENERAL MOVIE AGENTS
    # If NO game is active, use the correct Specialist Tool:

    1. **Search Agent (`search_movies`):**
       - Use for: "Find action movies from 1990", "Show me comedies rated 8+".

    2. **Recommendation Agent (`recommend_similar_movies`):**
       - Use for: "Movies like Inception", "If I like The Matrix...".

    3. **Analytics Agent (`get_movie_statistics`):**
       - Use for: "Highest rated movie?", "Average rating of 2024?", "How many movies?".

    4. **Person Agent (`search_by_person`):**
       - Use for: "Who directed Titanic?", "Movies starring Brad Pitt", "Bio of Nolan".

    5. **Comparison Agent (`compare_ratings`):**
       - Use for: "Critics vs Fans for Venom", "Compare ratings for The Room".

    6. **Trending Agent (`get_trending_movies`):**
       - Use for: "What's new?", "Trending now", "Latest releases".

    # üó£Ô∏è GUIDELINES:
    - **Voice Input:** Be tolerant of phonetic typos.
    - **No Results:** If a tool returns nothing, apologize.
    """),

    MessagesPlaceholder(variable_name="chat_history"),
    ("user", "{input}"),
    MessagesPlaceholder(variable_name="agent_scratchpad"),
])

agent = create_openai_tools_agent(llm, tools, prompt)

agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    verbose=True,
    handle_parsing_errors=True
)

print("‚úÖ Agent Orchestrator initialized with all 7 tools.")

"""### Test the Brain"""

# Test 1: Complex query (Search + Analytics)
print("--- TEST 1: Search & Analysis ---")
response = agent_executor.invoke({
    "input": "Find me high rated sci-fi movies from the 90s and tell me the average rating of what you found.",
    "chat_history": []
})
print(f"\nFinal Answer: {response['output']}\n")

# Test 2: Interactive Quiz
print("--- TEST 2: Interactive Quiz ---")
response = agent_executor.invoke({
    "input": "I'm bored. Give me a trivia challenge about directors.",
    "chat_history": []
})
print(f"\nFinal Answer: {response['output']}\n")

# Test 3: Person Search
print("--- TEST 3: Actor Search ---")
response = agent_executor.invoke({
    "input": "What movies has Christopher Nolan directed?",
    "chat_history": []
})
print(f"\nFinal Answer: {response['output']}\n")

"""# Section 6: Robustness & Error Handling

Building an Agentic system introduces multiple points of failure: API timeouts, malformed LLM outputs, or unexpected user inputs. To ensure reliability, I couldn't just let the Python kernel raise an exception and stop.

**My Safety Strategy:**
1.  **Custom Exceptions:** I defined specific error classes (`VisionError`, `ToolExecutionError`) to categorize failures.
2.  **The Safety Decorator:** I wrote a Python decorator, `@safe_agent_execution`, that wraps the main chat loop.
    * **Graceful Degradation:** If a tool fails, the decorator catches the error, logs the full stack trace for me (the developer), but returns a polite, helpful message to the user.
    * **Logging:** I configured the standard `logging` library to track these events for debugging.

## Implement Error Handling & Custom Exceptions (The Safety Net)
"""

# --- Logging Configuration ---
# I configured basic logging to capture timestamps and error levels.
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# --- Custom Exception Hierarchy ---
class AgentError(Exception):
    """Base class for all CineBot errors."""
    pass

class ToolExecutionError(AgentError):
    """Raised when a specific tool (e.g., Search, Stats) fails to run."""
    pass

class VisionError(AgentError):
    """Raised when the Gemini Vision API encounters an issue."""
    pass

class ParseError(AgentError):
    """Raised when the LLM produces output that cannot be parsed into a tool call."""
    pass

# --- The Safety Decorator ---
def safe_agent_execution(func: Callable) -> Callable:
    """
    A robust wrapper that guards the agent against crashes.
    It catches exceptions, logs them for the developer, and returns
    a safe, friendly message to the UI.
    """
    @wraps(func)
    def wrapper(*args, **kwargs) -> Any:
        try:
            return func(*args, **kwargs)

        except VisionError as e:
            logger.error(f"üëÅÔ∏è Vision Pipeline Failed: {e}")
            # Recover gracefully by informing the user
            return "I had trouble seeing that image. It might be a format I don't recognize. Could you try uploading a standard JPEG or PNG?"

        except ToolExecutionError as e:
            logger.error(f"üõ†Ô∏è Tool Execution Failed: {e}")
            return "I tried to look that up, but one of my search tools is acting up. Could you try rephrasing your request?"

        except ParseError as e:
            logger.error(f"üß† LLM Parsing Error: {e}")
            return "I'm having a bit of trouble understanding the context. Could you ask that in a slightly different way?"

        except Exception as e:
            # Catch-all for unexpected crashes
            logger.error(f"üí• Critical System Error: {e}")
            logger.debug(traceback.format_exc()) # Log full traceback for debugging
            return "Something unexpected went wrong on my end. I've logged the error for my developer to fix."

    return wrapper

print("‚úÖ Error handling system (Decorator + Custom Exceptions) active.")

"""## Edge Case Verification

To prove the robustness of the system, I designed a test suite targeting common failure modes. I wanted to verify that the agent behaves gracefully even when the user is difficult or the data is missing.

**The "Stress Test" Scenarios:**
1.  **The "Hallucination Trap":** Asking for a movie that definitely doesn't exist.
2.  **The "Typo Torrent":** Sending a query full of spelling errors.
3.  **The "Fact Check":** Providing contradictory information (e.g., wrong release year).
4.  **The "Out-of-Bounds":** Asking a non-movie question (e.g., sports results).
"""

def run_edge_case_tests():
    """
    Automated 'Stress Test' to verify the agent's robustness against
    bad data, typos, and out-of-scope queries.
    """
    print("\n" + "="*50)
    print("üß™ STARTING EDGE CASE STRESS TEST")
    print("="*50)

    scenarios = [
        {
            "name": "1. Non-Existent Movie",
            "input": "Find the movie 'The Ghost of 2099' starring invalid actor",
            "expected_behavior": "Should politely state no results found (no hallucinations)."
        },
        {
            "name": "2. Severe Typos",
            "input": "Reccomend som actoin mvoies form the 90 s please",
            "expected_behavior": "Should infer 'Action' and '1990s' correctly."
        },
        {
            "name": "3. Factual Inconsistency",
            "input": "Find 'The Matrix' released in 1950",
            "expected_behavior": "Should correct the year or find the 1999 movie anyway."
        },
        {
            "name": "4. Out-of-Scope Query",
            "input": "Who won the Super Bowl last year?",
            "expected_behavior": "Should politely refuse or pivot back to movies."
        }
    ]

    for test in scenarios:
        print(f"\nüîπ Scenario: {test['name']}")
        print(f"   Input: '{test['input']}'")

        try:
            start_t = time.time()
            # Invoke the agent directly (bypassing UI for pure logic test)
            response = agent_executor.invoke({
                "input": test['input'],
                "chat_history": []
            })
            duration = time.time() - start_t

            print(f"   Agent Response: \"{response['output']}\"")
            print(f"   Time: {duration:.2f}s")
            print(f"   ‚úÖ Result: PASSED (Handled without crash)")

        except Exception as e:
            print(f"   ‚ùå Result: FAILED (Crashed with error: {e})")

    print("\n" + "="*50)
    print("‚úÖ Stress Test Complete.")

# Run the verification
run_edge_case_tests()

"""# Section 7: User Interface & Advanced Features

A powerful agent needs an intuitive interface. I designed a custom UI using **Gradio** that supports three modes of interaction:
1.  **Text Chat:** The standard conversational interface.
2.  **Voice Interaction:** I injected custom JavaScript to access the browser's Web Speech API, allowing users to speak their queries naturally.
3.  **Visual Search (Advanced):** I implemented a pipeline to analyze movie posters using Google's Gemini Vision model. *(Note: This feature is fully coded below but currently disabled in the live demo due to Colab's asyncio loop constraints).*

I also applied custom CSS to give the application a polished, cinematic look.

## UI Assets & Configuration
"""

# --- UI Styling (Custom CSS) ---
movie_css = """
body, .gradio-app {
    background: linear-gradient(rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0.75)),
                url('https://wallpapers.com/images/hd/movie-collage-background-1920-x-1080-1-6b509z9c4s1e5c8t.jpg') no-repeat center center fixed !important;
    background-size: cover !important;
}

.gradio-container {
    margin-left: auto !important;
    margin-right: auto !important;
    width: 95% !important;
    max-width: 1000px !important; /* Optimized width for text chat */
    background-color: rgba(20, 20, 20, 0.85) !important;
    border: 1px solid #FFD700 !important;
    border-radius: 15px !important;
    box-shadow: 0 0 25px rgba(255, 215, 0, 0.2) !important;
    margin-top: 20px !important;
}

h1 {
    text-align: center !important;
    color: #FFD700 !important;
    font-family: 'Arial Black', sans-serif !important;
    text-transform: uppercase;
    text-shadow: 0px 0px 10px rgba(255, 215, 0, 0.5);
}

/* Make the Textbox Input look distinct */
textarea {
    font-size: 16px !important;
    background-color: #333 !important;
    color: white !important;
}

/* Chat Bubbles */
.message-row.user-row .message-bubble {
    background-color: #333333 !important;
    border: 1px solid #555;
    color: white !important;
}
.message-row.bot-row .message-bubble {
    background-color: #2b1f00 !important;
    border: 1px solid #FFD700;
    color: #FFF8DC !important;
}
"""

print("‚úÖ Cinematic CSS loaded.")

"""## Voice-Based Search
- Web Speech API integration (free)
- Speak your queries naturally
- Browser-based speech recognition
"""

# --- Voice Interaction (JavaScript Bridge) ---
# I wrote this JavaScript snippet to access the browser's microphone.
# It captures audio, converts it to text on the client side, and sends the string to Python.

voice_js = """
() => {
    // Check for browser compatibility
    if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
        alert("Web Speech API is not supported in this browser. Please use Chrome or Edge.");
        return "";
    }

    // Initialize Recognition
    var recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
    recognition.lang = "en-US";
    recognition.continuous = false;
    recognition.interimResults = false;

    return new Promise((resolve, reject) => {
        // UI Feedback
        console.log("üé§ Listening for voice input...");

        recognition.onstart = function() {
            console.log("Voice recognition started.");
        };

        recognition.onresult = function(event) {
            var transcript = event.results[0][0].transcript;
            console.log("Recognized: " + transcript);
            resolve(transcript);
        };

        recognition.onerror = function(event) {
            console.error("Speech recognition error", event.error);
            resolve(""); // Return empty string on error so app doesn't freeze
        };

        recognition.onend = function() {
            console.log("Voice recognition ended.");
        };

        recognition.start();
    });
}
"""
print("‚úÖ Voice Interaction (JS) configured.")

"""## Visual Search Implementation (Feature Documentation)

I developed a "Visual Search" pipeline that allows users to upload movie posters. The system uses Google's Gemini Vision model to analyze the image, extract metadata (Title, Year), and feed that context into the main Agent.

> **Note:** This feature is documented here in Markdown because running asynchronous image processing (Gemini API) inside a Gradio app within a Google Colab notebook causes `RuntimeError: asyncio` conflicts. The code below is production-ready for standalone deployments (e.g., Docker, Hugging Face Spaces).

#### Feature Overview:

- Upload movie posters to identify films

- Get recommendations based on visual style

- Analyze scenes to determine genre/mood

#### Technical Implementation:
The vision feature uses Google's Gemini Vision API (v2) with the following architecture:

1. Vision Model Setup (Memory-Based Processing):
We utilized PIL (Python Imaging Library) to handle images in memory, avoiding disk I/O conflicts common in Colab. The implementation targets the modern Gemini 2.0 model family.

```python
# --- VISION SETUP & HELPER FUNCTIONS ---

def setup_gemini():
    '''Authenticates Gemini using Colab Secrets or Env Vars.'''
    try:
        key = userdata.get('GOOGLE_API_KEY')
        genai.configure(api_key=key)
        return True
    except Exception as e:
        return False

def analyze_image_with_gemini(image_obj):
    '''
    Analyze images using Gemini Vision.
    Input: A PIL Image object (from memory), NOT a file path.
    '''
    try:
        # Resize to prevent API timeouts on free tier
        max_size = 1024
        if image_obj.width > max_size or image_obj.height > max_size:
            image_obj.thumbnail((max_size, max_size))
            
        # Target the latest supported model (Gemini 2.0 Flash)
        model = genai.GenerativeModel('gemini-2.0-flash')
        prompt = "Describe this movie image in detail. Identify title, year, and genre."
        
        response = model.generate_content([prompt, image_obj])
        return f"[Image Analysis: {response.text}]"
    except Exception as e:
        raise VisionError(str(e))
```
2. Agent Integration Logic:
The Chat Logic was designed to accept an optional image_obj, process it, and append the analysis to the user's text prompt before sending it to the Agent.

```python
@safe_agent_execution
def chat_logic(user_text, history, image_obj=None):
    # 1. Handle Vision
    vision_context = ""
    if image_obj is not None:
        try:
            vision_context = analyze_image_with_gemini(image_obj)
        except Exception as e:
            vision_context = f"[System Note: Image analysis failed: {str(e)}]"

    # 2. Construct Augmented Prompt
    full_query = user_text
    if vision_context:
        full_query = f"{user_text}\n\n{vision_context}"

    # 3. Execute Agent with Augmented Prompt
    response = agent_executor.invoke({
        "input": full_query,
        "chat_history": langchain_history
    })
    return "", updated_history
```
3. UI Implementation (Gradio):
The UI was configured with type="pil" to ensure memory-only processing and show_progress="hidden" to attempt to mitigate event loop conflicts.

```python
# UI Component Configuration
with gr.Column(scale=1):
    gr.Markdown("### Visual Search")
    image_in = gr.Image(
        type="pil",              # Critical: Use memory object, not filepath
        label="Upload Poster",
        height=300,
        interactive=True
    )

# Wiring
submit_btn.click(
    chat_logic,
    [msg, chatbot, image_in],    # Pass image input
    [msg, chatbot],
    show_progress="hidden"       # Disable progress bar to prevent crash
)
```
**Why Disabled in Colab:**

1.Event Loop Conflict:

 Even with type="pil", Gradio's internal upload handling fights with the Colab notebook's event loop when debug=True, causing RuntimeError.
2. Environment Constraints:

 The combination of Gradio 5.x, Colab's async environment, and the Gemini API client requires a dedicated server environment (like Hugging Face Spaces) for 100% stability.

**Production Deployment:**

This feature works correctly when deployed to Hugging Face Spaces or a standalone Python server, where the event loop is not shared with a notebook kernel.

## Integrated Chat Logic
This is the engine room of the interface. I implemented a single function, chat_logic, to serve as the bridge between the frontend (Gradio) and the backend (Agent Orchestrator). I wrapped it with my @safe_agent_execution decorator to ensure that even if the Agent fails, the UI remains responsive.
"""

@safe_agent_execution
def chat_logic(user_text, history):
    """
    The central handler for the UI. It processes both typed text and
    transcribed voice input, maintains conversation history, and
    connects to the Agent Orchestrator.
    """
    # Guard clause: If the input is empty (e.g., accidental click), do nothing
    if not user_text or not user_text.strip():
        return "", history
    # Context Management
    # I convert Gradio's history (list of lists) into LangChain's message objects
    # so the Agent remembers the conversation context.
    langchain_history = []
    if history:
        for human_msg, ai_msg in history:
            langchain_history.append(HumanMessage(content=str(human_msg)))
            langchain_history.append(AIMessage(content=str(ai_msg)))

    # Agent Execution
    # I print the input to the logs for debugging purposes
    print(f"üí¨ Processing Input: {user_text[:50]}...")

    # The @safe_agent_execution decorator protects this call from crashing
    response = agent_executor.invoke({
        "input": user_text,
        "chat_history": langchain_history
    })

    bot_response = response["output"]

    # Update History
    # I append the new interaction to the UI history and clear the input box
    updated_history = history + [[user_text, bot_response]]

    return "", updated_history

print("‚úÖ Chat Logic linked to Orchestrator.")

"""## Build & Launch Gradio Interface

"""

# --- Application Launch ---

# Apply nest_asyncio to allow Gradio to run inside the Colab notebook loop
nest_asyncio.apply()

with gr.Blocks(theme=gr.themes.Soft(), css=movie_css) as demo:

    # Header
    gr.Markdown(
        """
        # üé¨ IMDb CineBot
        ### Your AI Movie Expert | Powered by RAG & Voice
        """
    )

    with gr.Row():
        # Left Column: Chat Interface
        with gr.Column(scale=3): # Increased scale for main chat
            chatbot = gr.Chatbot(
                height=550,
                label="Conversation",
                bubble_full_width=False,
                avatar_images=(None, "https://cdn-icons-png.flaticon.com/512/4712/4712035.png")
            )

            # Input Area (Expanded)
            with gr.Row():
                msg = gr.Textbox(
                    label="Your Question",
                    placeholder="Ask about movies, actors, or try 'Give me a quiz'...",
                    scale=8,        # Make textbox much wider than buttons
                    autofocus=True,
                    lines=2         # Taller box for easier typing
                )
                voice_btn = gr.Button("üé§ Speak", variant="secondary", scale=1)

            # Control Buttons
            with gr.Row():
                submit_btn = gr.Button("üöÄ Send Request", variant="primary")
                clear_btn = gr.Button("üóëÔ∏è Clear Chat", variant="stop")

        # Right Column: Persistent Instructions
        with gr.Column(scale=1):
            gr.Markdown(
                """
                ###Quick Tips

                **Search**
                > "Find 90s action movies rated 8+"

                **Analytics**
                > "What is the average rating of horror movies?"

                **Trivia Game**
                > "Start a movie quiz"

                **üé§ Voice Command**
                > Click **Speak** and allow microphone access.
                """
            )

    # --- Event Listeners ---

    # 1. Text Submission (Enter Key & Button)
    msg.submit(chat_logic, [msg, chatbot], [msg, chatbot])
    submit_btn.click(chat_logic, [msg, chatbot], [msg, chatbot])

    # 2. Voice Submission (JavaScript Trigger)
    voice_btn.click(
        fn=None,
        inputs=[],
        outputs=[msg],
        js=voice_js
    )

    # 3. Clear History
    clear_btn.click(lambda: None, None, chatbot, queue=False)

# Launch the App
print("Launching CineBot Interface...")
demo.launch(debug=True, share=True, allowed_paths=["/tmp"])

"""## Test Multi-Agent System"""

agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    verbose=True,
    handle_parsing_errors=True,
    return_intermediate_steps=True
)

def run_system_diagnostic():
    """
    Comprehensive System Diagnostic.
    Phase 1: Verifies individual Agent capabilities (Search, Stats, Recs, etc.).
    Phase 2: Verifies Trivia Game State Machine (Logic & Memory persistence).
    """
    print("\n" + "="*60)
    print("SYSTEM DIAGNOSTIC SUITE")
    print("="*60 + "\n")

    stats = {"passed": 0, "failed": 0, "total": 0}

    # =========================================================================
    # PHASE 1: SPECIALIST AGENT CAPABILITIES
    # =========================================================================
    print(f"PHASE 1: SPECIALIST AGENT TESTS")
    print(f"{'-'*60}")

    agent_test_cases = [
        {
            "name": "Search Agent",
            "query": "Find action movies from the 1990s rated above 8.0",
            # Check for year (199x) OR genre match
            "check": lambda out: "199" in out or "Action" in out,
            "desc": "Filtering by Year/Genre/Rating"
        },
        {
            "name": "Recommendation Agent",
            "query": "I liked Inception, what should I watch next?",
            # Expect a decent length response with list items
            "check": lambda out: len(out) > 50,
            "desc": "Similarity Search functionality"
        },
        {
            "name": "Analytics Agent",
            "query": "What is the average rating of movies released in 2024?",
            # Check for digits or specific calculation phrasing
            "check": lambda out: any(char.isdigit() for char in out),
            "desc": "Statistical aggregation"
        },
        {
            "name": "Person Agent",
            "query": "Tell me about movies directed by Christopher Nolan",
            "check": lambda out: "Nolan" in out or "Inception" in out,
            "desc": "Person-based filtering"
        },
        {
            "name": "Comparison Agent",
            "query": "Compare critic vs user ratings for The Dark Knight",
            # Checks for 'rated', '/10', or 'consensus'
            "check": lambda out: "rated" in out.lower() or "/10" in out or "consensus" in out.lower(),
            "desc": "Multi-metric comparison"
        },
        {
            "name": "Trending Agent",
            "query": "What are the trending movies right now?",
            "check": lambda out: len(out) > 20,
            "desc": "Fetch latest entries"
        },
        {
            "name": "Voice/Fuzzy Input",
            "query": "find movies with leonardo da caprio",
            "check": lambda out: "Leonardo" in out or "Titanic" in out,
            "desc": "Typo tolerance & Entity extraction"
        },
        {
            "name": "Safety/Edge Case",
            "query": "Find movies about flying spaghetti monsters in space 1920",
            # Checks for 'find', 'couldn't', 'sorry', or 'no movies'
            "check": lambda out: any(x in out.lower() for x in ["sorry", "no movies", "find", "couldn't"]),
            "desc": "Handling empty results gracefully"
        }
    ]

    for test in agent_test_cases:
        print(f"\n[TEST] {test['name']}")
        print(f"Query: '{test['query']}'")

        start_t = time.time()
        try:
            # Run Agent
            response = agent_executor.invoke({"input": test['query'], "chat_history": []})
            output = response['output']
            duration = time.time() - start_t

            # Verify
            if output and test['check'](output):
                print(f"[PASS] Success ({duration:.2f}s)")
                stats["passed"] += 1
            else:
                print(f"[FAIL] Output did not match criteria.")
                print(f"Output: {output[:100]}...")
                stats["failed"] += 1
        except Exception as e:
            print(f"[ERROR] Exception occurred: {str(e)}")
            stats["failed"] += 1

        stats["total"] += 1

    # =========================================================================
    # PHASE 2: TRIVIA GAME LOGIC (STATE MACHINE)
    # =========================================================================
    print(f"\n\nPHASE 2: GAME LOGIC & STATE MACHINE")
    print(f"{'-'*60}")

    game_history = []
    current_key = ""

    # --- STEP 1: START GAME ---
    print("\n[STEP 1] Start Game (Trigger Tool)")
    try:
        res1 = agent_executor.invoke({"input": "Let's play a quiz", "chat_history": game_history})

        # Check Tool Usage
        steps = res1.get("intermediate_steps", [])
        if steps and steps[0][0].tool == "generate_movie_quiz":
            # Extract Key
            match = re.search(r"\|\|INTERNAL_ANSWER_KEY: (.*?)\|\|", steps[0][1])
            if match:
                current_key = match.group(1)
                print(f"[PASS] Tool called. Key stored: '{current_key}'")
                stats["passed"] += 1

                # Update History
                game_history.append(HumanMessage(content="Let's play a quiz"))
                game_history.append(AIMessage(content=res1["output"]))
            else:
                print("[FAIL] Tool called but NO key found in output.")
                stats["failed"] += 1
        else:
            print("[FAIL] Agent did not call 'generate_movie_quiz'.")
            stats["failed"] += 1
    except Exception as e:
        print(f"[ERROR] {str(e)}")
        stats["failed"] += 1
    stats["total"] += 1

    # --- STEP 2: WRONG ANSWER (Verify Freeze) ---
    if current_key:
        print("\n[STEP 2] Wrong Answer Test (Verify 'Freeze')")
        try:
            res2 = agent_executor.invoke({"input": "Bruce Willis", "chat_history": game_history})

            # Check 1: NO TOOLS ALLOWED
            if len(res2.get("intermediate_steps", [])) == 0:
                # Check 2: Output contains "Wrong"
                if "wrong" in res2["output"].lower():
                    print("[PASS] No tools called + Correctly graded WRONG.")
                    stats["passed"] += 1
                else:
                    print(f"[FAIL] Agent did not say 'Wrong'. Output: {res2['output']}")
                    stats["failed"] += 1
            else:
                print(f"[FAIL] Agent called a tool! (It should have frozen).")
                stats["failed"] += 1

            game_history.append(HumanMessage(content="Bruce Willis"))
            game_history.append(AIMessage(content=res2["output"]))
        except Exception as e:
            print(f"[ERROR] {str(e)}")
            stats["failed"] += 1
        stats["total"] += 1

    # --- STEP 3: NEXT QUESTION (Trigger Tool) ---
    print("\n[STEP 3] Request Next Question")
    try:
        res3 = agent_executor.invoke({"input": "Next question please", "chat_history": game_history})

        steps = res3.get("intermediate_steps", [])
        if steps and steps[0][0].tool == "generate_movie_quiz":
            match = re.search(r"\|\|INTERNAL_ANSWER_KEY: (.*?)\|\|", steps[0][1])
            if match:
                current_key = match.group(1)
                print(f"[PASS] New Question generated. New Key: '{current_key}'")
                stats["passed"] += 1

                game_history.append(HumanMessage(content="Next question please"))
                game_history.append(AIMessage(content=res3["output"]))
            else:
                print("[FAIL] No key in new question.")
                stats["failed"] += 1
        else:
            print("[FAIL] Agent failed to generate new quiz.")
            stats["failed"] += 1
    except Exception as e:
        print(f"[ERROR] {str(e)}")
        stats["failed"] += 1
    stats["total"] += 1

    # --- STEP 4: CORRECT ANSWER (Verify Freeze + Grade) ---
    if current_key:
        print(f"\n[STEP 4] Correct Answer Test (Sending '{current_key}')")
        try:
            res4 = agent_executor.invoke({"input": current_key, "chat_history": game_history})

            if len(res4.get("intermediate_steps", [])) == 0:
                if "correct" in res4["output"].lower():
                    print("[PASS] No tools called + Correctly graded CORRECT.")
                    stats["passed"] += 1
                else:
                    print(f"[FAIL] Agent did not say 'Correct'. Output: {res4['output']}")
                    stats["failed"] += 1
            else:
                print("[FAIL] Agent called a tool instead of grading.")
                stats["failed"] += 1
        except Exception as e:
            print(f"[ERROR] {str(e)}")
            stats["failed"] += 1
        stats["total"] += 1

    # =========================================================================
    # SUMMARY
    # =========================================================================
    print("\n" + "="*60)
    print("FINAL DIAGNOSTIC REPORT")
    print("="*60)
    score = (stats['passed'] / stats['total']) * 100
    print(f"Total Tests: {stats['total']}")
    print(f"Passed:      {stats['passed']}")
    print(f"Failed:      {stats['failed']}")
    print(f"Score:       {score:.1f}%")

    if score == 100:
        print("\nRESULT: SYSTEM READY")
    elif score >= 80:
        print("\nRESULT: SYSTEM STABLE (Minor issues)")
    else:
        print("\nRESULT: CRITICAL FAILURES")
    print("="*60 + "\n")

# --- EXECUTE TEST SUITE ---
run_system_diagnostic()

"""# Section 8: Evaluation & Testing

# Section 8: Evaluation & Testing

Building the bot is only half the battle; proving it works is the other. I adopted a two-pronged evaluation strategy to validate both the user experience and the technical accuracy.

## 8.1 Evaluation Methodology

**1. Qualitative Analysis (Functional Testing)**
I designed a test suite to verify that the "Orchestrator" correctly routes different intents to the right tools.
* **Title Search:** Can it find a specific movie?
* **Description Search:** Can it find a movie based on a vague plot summary?
* **Entity Search:** Can it filter by specific Actors or Directors?
* **Genre/Metadata:** Can it handle complex filters like "Action movies from the 90s"?

**2. Quantitative Analysis (Retrieval Metrics)**
To measure the RAG pipeline's raw performance, I calculated standard Information Retrieval metrics:
* **Precision@K:** Of the top K retrieved movies, how many are relevant? (High precision = few wrong answers).
* **Recall@K:** Of all relevant movies in the DB, how many did we find? (High recall = few missed answers).

## Test Various Query Types
   - Title-based
   - Description-based
   - Actor-based
   - Genre-based
"""

# --- Qualitative Evaluation: Query Types ---

def test_query_types():
    """
    Runs a functional test across distinct query categories to ensure
    the Agent's routing logic handles diverse inputs correctly.
    """
    print("--- üß™ STARTING QUALITATIVE EVALUATION ---")

    # Define test cases for each category
    test_queries = {
        "Title-Based": "Who directed the movie Inception?",
        "Description-Based": "Find me a movie about a thief who enters people's dreams to steal secrets.",
        "Actor-Based": "What movies has Leonardo DiCaprio starred in?",
        "Genre-Based": "Recommend some high-rated horror movies from the 2010s."
    }

    for category, query in test_queries.items():
        print(f"\n[Testing Category: {category}]")
        print(f"Query: '{query}'")

        start_time = time.time()
        try:
            # Invoke Agent
            response = agent_executor.invoke({"input": query, "chat_history": []})
            output = response['output']
            duration = time.time() - start_time

            # Print brief result
            print(f"Response Time: {duration:.2f}s")
            print(f"Result Snippet: {output[:150]}...")
            print("‚úÖ PASS")
        except Exception as e:
            print(f"‚ùå FAIL: {e}")

# Run the test
test_query_types()

"""## Calculate Quantitative Metrics
   - Precision@K, Recall@K if applicable
"""

# --- Quantitative Evaluation: Offline Metrics ---

def calculate_retrieval_metrics():
    """
    Calculates Precision@K and Recall@K for the RAG retriever.
    I compare the retriever's results against a known 'Ground Truth' subset.
    """
    print("\n--- STARTING QUANTITATIVE EVALUATION ---")

    # 1. Define Ground Truth (The 'Correct' Answers for our dataset)
    # I selected these specific examples because I know they exist in the DB.
    ground_truth = {
        "Christopher Nolan": ["Inception", "The Dark Knight", "Interstellar", "The Prestige", "Memento", "The Dark Knight Rises", "Oppenheimer"],
        "Toy Story": ["Toy Story", "Toy Story 3"],
        "Frank Darabont": ["The Shawshank Redemption", "The Green Mile"]
    }

    total_precision = 0
    total_recall = 0
    k = 5  # Top 5 results

    for query, expected_movies in ground_truth.items():
        print(f"\nQuery: '{query}'")

        # Retrieve documents directly (Bypassing Agent to test RAG only)
        retrieved_docs = retriever.invoke(query)

        # Extract titles from retrieved docs
        retrieved_titles = []
        for doc in retrieved_docs[:k]:
            content = doc.page_content
            # Check if any expected title is inside the content string
            for expected in expected_movies:
                if expected in content and expected not in retrieved_titles:
                    retrieved_titles.append(expected)

        # Calculate Metrics
        relevant_retrieved = len(retrieved_titles)

        # Precision = (Relevant Retrieved) / K
        precision = relevant_retrieved / k

        # Recall = (Relevant Retrieved) / (Total Relevant in GT)
        total_expected = len(expected_movies)
        recall = relevant_retrieved / total_expected if total_expected > 0 else 0

        print(f"  Expected: {len(expected_movies)} | Retrieved: {relevant_retrieved}")
        print(f"  Precision@{k}: {precision:.2f}")
        print(f"  Recall@{k}:    {recall:.2f}")

        total_precision += precision
        total_recall += recall

    # Final Averages
    avg_precision = total_precision / len(ground_truth)
    avg_recall = total_recall / len(ground_truth)

    print("-" * 40)
    print(f"AVERAGE PRECISION@{k}: {avg_precision:.2f}")
    print(f"AVERAGE RECALL@{k}:    {avg_recall:.2f}")
    print("-" * 40)

# Run the metrics
calculate_retrieval_metrics()

"""## 8.4 Results & Analysis

### System Performance Overview
The evaluation confirms that the IMDb CineBot functions as a robust Multi-Agent RAG system. By separating concerns into specialized agents, the system achieves high accuracy across diverse query types.

**1. Retrieval Metrics (Precision & Recall)**
My offline evaluation yielded an **Average Precision@5 of ~0.33** and **Recall@5 of ~0.48**.
* *Analysis:* These numbers are expected for a small subset of data. The "Recall" is lower because complex queries (like "All Nolan Movies") often have more correct answers than the top-5 retrieval window allows. Increasing `k` to 10 would improve this.

**2. Latency & User Experience**
* **Response Time:** Averaged ~2-4 seconds per query, which is acceptable for a chained LLM call.
* **Game Logic:** The "Parrot Protocol" successfully prevented hallucinations during trivia rounds, maintaining 100% grading consistency in my tests.

**3. Conclusion**
The system successfully bridges the gap between structured database queries and unstructured conversation. Future optimizations would focus on scaling the vector store to the full dataset to improve Recall scores.

# Section 9: Future Improvements

# Section 9: Future Improvements & Roadmap

While the current version of IMDb CineBot is a fully functional prototype, I have identified several architectural enhancements to elevate it to a production-grade application.

## 9.1 Re-enabling Multimodal Search
* **Current Constraint:** I successfully implemented the Visual Search pipeline (documented in Section 7) but had to disable it in the live demo due to `asyncio` event loop conflicts specific to the Google Colab environment.
* **The Fix:** Deploying the application to a containerized environment (like **Docker** on **AWS Fargate** or **Hugging Face Spaces**) would resolve these kernel conflicts. This would allow users to upload movie posters for instant identification without stability risks.

## 9.2 Mobile Application Deployment
* **Current State:** The interface currently runs as a responsive web app via Gradio.
* **Future Plan:** I plan to decouple the backend logic (LangChain) from the frontend by exposing the agent as a **FastAPI** microservice. I would then build a native mobile app using **React Native** or **Flutter**. This would unlock native hardware features, such as waking the bot with a "Hey CineBot" voice command or scanning a DVD cover directly with the camera.

## 9.3 Cost Optimization with Open Source LLMs
* **Current State:** The system relies on OpenAI's GPT-3.5, which incurs per-token API costs.
* **Future Plan:** To improve data privacy and eliminate operating costs, I plan to fine-tune an open-source model like **Llama 3 (8B)** or **Mistral 7B**. By using **4-bit quantization**, I could run these models locally on consumer hardware, making the CineBot entirely self-contained and free to run.

## 9.4 Real-Time Data Streams
* **Current State:** The "Trending Agent" currently relies on a static snapshot of data (the `movies_with_posters.csv` file).
* **Future Plan:** I would replace the static dataset with live API calls to **TMDB (The Movie Database)**. By creating a custom LangChain Tool that hits the `GET /movie/popular` endpoint, the bot could provide up-to-the-minute box office numbers and showtimes for movies released today.

- Multimodal search
   - Mobile app
   - Alternative LLMs (Llama)
   - Real-time trending

# Test the performance of your bot with various test cases and refine your code!
"""